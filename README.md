# ğŸ”¥ Inferno - Your Personal AI Infrastructure

> **Run any AI model locally with enterprise-grade performance and privacy**

[![Build Status](https://github.com/ringo380/inferno/workflows/CI/badge.svg)](https://github.com/ringo380/inferno/actions)
[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue.svg)](LICENSE)
[![Rust Version](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://rustlang.org)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://hub.docker.com/r/inferno/inferno)

Inferno is a **production-ready AI inference server** that runs entirely on your hardware. Think of it as your private ChatGPT that works offline, supports any model format, and gives you complete control over your AI infrastructure.

## ğŸ¯ Why Inferno?

### **ğŸ”’ Privacy First**
- **100% Local**: All processing happens on your hardware
- **No Cloud Dependency**: Works completely offline
- **Your Data Stays Yours**: Zero telemetry or external data transmission

### **ğŸš€ Universal Model Support**
- **GGUF Models**: Native support for Llama, Mistral, CodeLlama, and more
- **ONNX Models**: Run models from PyTorch, TensorFlow, scikit-learn
- **Format Conversion**: Convert between GGUF â†” ONNX â†” PyTorch â†” SafeTensors
- **Auto-Optimization**: Automatic quantization and hardware optimization

### **âš¡ Enterprise Performance**
- **GPU Acceleration**: NVIDIA, AMD, Apple Silicon, Intel support
- **Smart Caching**: Remember previous responses for instant results
- **Batch Processing**: Handle thousands of requests efficiently
- **Load Balancing**: Distribute work across multiple models/GPUs

### **ğŸ”§ Developer Friendly**
- **OpenAI-Compatible API**: Drop-in replacement for ChatGPT API
- **REST & WebSocket**: Standard APIs plus real-time streaming
- **Multiple Languages**: Python, JavaScript, Rust, cURL examples
- **Docker Ready**: One-command deployment
- **Smart CLI**: Typo detection, helpful error messages, setup guidance

## ğŸš€ Quick Start

```bash
# Build from source
git clone https://github.com/ringo380/inferno.git
cd inferno
cargo build --release

# List available models
./target/release/inferno models list

# Run inference
./target/release/inferno run --model MODEL_NAME --prompt "Your prompt here"

# Start HTTP API server
./target/release/inferno serve

# Launch desktop app
npm run tauri dev
```

## âœ¨ Key Features

### **ğŸ§  AI Backends**
- âœ… **Real GGUF Support**: Full llama.cpp integration
- âœ… **Real ONNX Support**: Production ONNX Runtime with GPU acceleration
- âœ… **Model Conversion**: Real-time format conversion with optimization
- âœ… **Quantization**: Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, F16, F32 support

### **ğŸ¢ Enterprise Features**
- âœ… **Authentication**: JWT tokens, API keys, role-based access
- âœ… **Monitoring**: Prometheus metrics, OpenTelemetry tracing
- âœ… **Audit Logging**: Encrypted logs with multi-channel alerting
- âœ… **Batch Processing**: Cron scheduling, retry logic, job dependencies
- âœ… **Caching**: Multi-tier caching with compression and persistence
- âœ… **Load Balancing**: Distribute inference across multiple backends

### **ğŸ”Œ APIs & Integration**
- âœ… **OpenAI Compatible**: Use existing ChatGPT client libraries
- âœ… **REST API**: Standard HTTP endpoints for all operations
- âœ… **WebSocket**: Real-time streaming and bidirectional communication
- âœ… **CLI Interface**: 40+ commands for all AI/ML operations
- âœ… **Desktop App**: Cross-platform Tauri application

## ğŸ—ï¸ Architecture

Built with a modular, trait-based architecture supporting pluggable backends:

```
src/
â”œâ”€â”€ main.rs           # CLI entry point
â”œâ”€â”€ lib.rs            # Library exports
â”œâ”€â”€ config.rs         # Configuration management
â”œâ”€â”€ backends/         # AI model execution backends
â”œâ”€â”€ cli/              # 40+ CLI command modules
â”œâ”€â”€ api/              # HTTP/WebSocket APIs
â”œâ”€â”€ batch/            # Batch processing system
â”œâ”€â”€ models/           # Model discovery and metadata
â””â”€â”€ [Enterprise]      # Advanced production features
```

## ğŸ”§ Configuration

Create `inferno.toml`:

```toml
# Basic settings
models_dir = "/path/to/models"
log_level = "info"

[server]
bind_address = "0.0.0.0"
port = 8080

[backend_config]
gpu_enabled = true
context_size = 4096
batch_size = 64

[cache]
enabled = true
compression = "zstd"
max_size_gb = 10
```

## ğŸ› ï¸ Development

See [CLAUDE.md](CLAUDE.md) for comprehensive development documentation.

```bash
# Run tests
cargo test

# Format code
cargo fmt

# Run linter
cargo clippy

# Full verification
./verify.sh
```

## ğŸ“„ License

Licensed under either of:
- Apache License, Version 2.0
- MIT License

---

<div align="center">

**ğŸ”¥ Ready to take control of your AI infrastructure? ğŸ”¥**

*Built with â¤ï¸ by the open source community*

</div>