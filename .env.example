# Inferno v0.8.0 Environment Configuration
# Copy this file to .env and update values for your environment
# DO NOT commit .env to version control - it's gitignored for security

# ============================================================================
# API SERVER CONFIGURATION
# ============================================================================

# Bind address and port (default: 0.0.0.0:8000)
INFERNO_BIND_ADDRESS=0.0.0.0
INFERNO_BIND_PORT=8000

# Workers for async runtime (default: number of CPU cores)
INFERNO_WORKER_THREADS=4

# Request timeout in seconds (default: 300)
INFERNO_REQUEST_TIMEOUT=300

# ============================================================================
# MODEL & INFERENCE CONFIGURATION
# ============================================================================

# Directory for model files
INFERNO_MODELS_DIR=./data/models

# Cache directory (for inference caching)
INFERNO_CACHE_DIR=./data/cache

# Configuration directory
INFERNO_CONFIG_DIR=./data/config

# Queue state directory (for queue persistence)
INFERNO_QUEUE_DIR=./data/queue

# Maximum model size in GB (default: 50)
INFERNO_MAX_MODEL_SIZE_GB=50

# Context size for models (default: 2048)
INFERNO_CONTEXT_SIZE=2048

# Batch size for inference (default: 32)
INFERNO_BATCH_SIZE=32

# GPU enablement (true/false, default: true on supported platforms)
INFERNO_GPU_ENABLED=true

# ============================================================================
# QUEUE & SCHEDULING CONFIGURATION
# ============================================================================

# Max pending messages in queue (default: 1000)
INFERNO_QUEUE_MAX_PENDING=1000

# Queue moderate backpressure threshold % (default: 70)
INFERNO_QUEUE_MODERATE_THRESHOLD=70

# Queue critical backpressure threshold % (default: 90)
INFERNO_QUEUE_CRITICAL_THRESHOLD=90

# Max concurrent workers (default: number of CPU cores * 2, max 64)
INFERNO_MAX_WORKERS=8

# Queue checkpoint interval in seconds (default: 30)
INFERNO_QUEUE_CHECKPOINT_INTERVAL=30

# ============================================================================
# PROFILING & MONITORING CONFIGURATION
# ============================================================================

# Enable profiling (default: true)
INFERNO_PROFILING_ENABLED=true

# Max profiles in memory (default: 1000)
INFERNO_PROFILING_MAX_PROFILES=1000

# Inference timeout in seconds (default: 300)
INFERNO_INFERENCE_TIMEOUT=300

# Token timeout in seconds (default: 30)
INFERNO_TOKEN_TIMEOUT=30

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: trace, debug, info, warn, error (default: info)
INFERNO_LOG_LEVEL=info

# Log format: pretty, json, compact (default: pretty)
INFERNO_LOG_FORMAT=pretty

# Log file (optional, if empty logs to console only)
INFERNO_LOG_FILE=./data/logs/inferno.log

# ============================================================================
# CACHE CONFIGURATION
# ============================================================================

# Cache strategy: memory, disk, hybrid (default: hybrid)
INFERNO_CACHE_STRATEGY=hybrid

# Cache size in MB (default: 500)
INFERNO_CACHE_SIZE_MB=500

# Cache TTL in seconds (default: 3600)
INFERNO_CACHE_TTL=3600

# Enable compression for cached responses (default: true)
INFERNO_CACHE_COMPRESSION_ENABLED=true

# ============================================================================
# STREAMING CONFIGURATION
# ============================================================================

# Token batch size (default: 3)
INFERNO_TOKEN_BATCH_SIZE=3

# Token batch max wait in ms (default: 50)
INFERNO_TOKEN_BATCH_MAX_WAIT_MS=50

# Keep-alive interval in seconds (default: 30)
INFERNO_KEEPALIVE_INTERVAL=30

# Compression format: none, gzip, deflate, brotli (default: auto)
INFERNO_COMPRESSION_FORMAT=auto

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

# Enable HTTPS (requires certificate and key files)
INFERNO_HTTPS_ENABLED=false

# HTTPS certificate file path (if HTTPS_ENABLED=true)
INFERNO_HTTPS_CERT=/path/to/cert.pem

# HTTPS key file path (if HTTPS_ENABLED=true)
INFERNO_HTTPS_KEY=/path/to/key.pem

# Enable authentication (default: false)
INFERNO_AUTH_ENABLED=false

# Auth secret key (required if AUTH_ENABLED=true)
INFERNO_AUTH_SECRET_KEY=your-secret-key-here

# ============================================================================
# MONITORING & OBSERVABILITY
# ============================================================================

# Prometheus metrics endpoint (default: /metrics)
INFERNO_METRICS_ENABLED=true
INFERNO_METRICS_PORT=9090

# Error tracking (Sentry) DSN (optional)
# INFERNO_SENTRY_DSN=https://key@sentry.io/project-id

# ============================================================================
# DEPLOYMENT CONFIGURATION
# ============================================================================

# Deployment environment: development, staging, production (default: development)
INFERNO_ENV=development

# Instance name for identification in logs
INFERNO_INSTANCE_NAME=inferno-1

# Region (for multi-region deployments)
INFERNO_REGION=us-east-1

# ============================================================================
# NOTES
# ============================================================================

# Security Best Practices:
# 1. Never commit this file with real values
# 2. Use .env.local for local development
# 3. Use secrets management in production (AWS Secrets Manager, Vault, etc.)
# 4. Rotate AUTH_SECRET_KEY regularly
# 5. Use HTTPS in production (HTTPS_ENABLED=true)

# Performance Tuning:
# - Increase WORKER_THREADS for high-throughput scenarios
# - Adjust CACHE_SIZE_MB based on available memory
# - Set TOKEN_BATCH_SIZE=1 for lower latency
# - Set TOKEN_BATCH_SIZE=10+ for higher throughput

# Monitoring & Debugging:
# - Set LOG_LEVEL=debug for detailed logging
# - Enable PROFILING_ENABLED=true for performance analysis
# - Check METRICS_ENABLED for Prometheus integration
# - Use SENTRY_DSN for error tracking
